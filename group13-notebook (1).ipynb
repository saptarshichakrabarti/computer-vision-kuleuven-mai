{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de53f648",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-14T21:57:48.825667Z",
     "iopub.status.busy": "2024-04-14T21:57:48.825118Z",
     "iopub.status.idle": "2024-04-14T21:58:04.817704Z",
     "shell.execute_reply": "2024-04-14T21:58:04.816614Z"
    },
    "papermill": {
     "duration": 16.024572,
     "end_time": "2024-04-14T21:58:04.820422",
     "exception": false,
     "start_time": "2024-04-14T21:57:48.795850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 21:57:53.704421: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-14 21:57:53.704533: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-14 21:57:53.851787: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "import io # Input/Output Module\n",
    "import os # OS interfaces\n",
    "import cv2 # OpenCV package\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from urllib import request # module for opening HTTP requests\n",
    "from matplotlib import pyplot as plt # Plotting library\n",
    "from PIL import Image\n",
    "\n",
    "from urllib import request # module for opening HTTP requests\n",
    "from matplotlib import pyplot as plt # Plotting library\n",
    "import sklearn\n",
    "from sklearn import decomposition\n",
    "from skimage.feature import hog\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import mode\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c8d03c",
   "metadata": {
    "papermill": {
     "duration": 0.021813,
     "end_time": "2024-04-14T21:58:04.865028",
     "exception": false,
     "start_time": "2024-04-14T21:58:04.843215",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%; height:140px\">\n",
    "    <img src=\"https://www.kuleuven.be/internationaal/thinktank/fotos-en-logos/ku-leuven-logo.png/image_preview\" width = 300px, heigh = auto align=left>\n",
    "</div>\n",
    "\n",
    "\n",
    "KUL H02A5a Computer Vision: Group Assignment 1\n",
    "---------------------------------------------------------------\n",
    "Student numbers: <span style=\"color:red\">r0999063, r0967112, r0753150, r0775439, r0967921</span>.\n",
    "\n",
    "The goal of this assignment is to explore more advanced techniques for constructing features that better describe objects of interest and to perform face recognition using these features. This assignment will be delivered in groups of 5 (either composed by you or randomly assigned by your TA's).\n",
    "\n",
    "In this assignment you are a group of computer vision experts that have been invited to ECCV 2021 to do a tutorial about  \"Feature representations, then and now\". To prepare the tutorial you are asked to participate in a kaggle competition and to release a notebook that can be easily studied by the tutorial participants. Your target audience is: (master) students who want to get a first hands-on introduction to the techniques that you apply.\n",
    "\n",
    "---------------------------------------------------------------\n",
    "This notebook is structured as follows:\n",
    "0. Data loading & Preprocessing\n",
    "1. Feature Representations\n",
    "2. Evaluation Metrics \n",
    "3. Classifiers\n",
    "4. Experiments\n",
    "5. Publishing best results\n",
    "6. Discussion\n",
    "\n",
    "Make sure that your notebook is **self-contained** and **fully documented**. Walk us through all steps of your code. Treat your notebook as a tutorial for students who need to get a first hands-on introduction to the techniques that you apply. Provide strong arguments for the design choices that you made and what insights you got from your experiments. Make use of the *Group assignment* forum/discussion board on Toledo if you have any questions.\n",
    "\n",
    "Fill in your student numbers above and get to it! Good luck! \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> This notebook is just a example/template, feel free to adjust in any way you please! Just keep things organised and document accordingly!\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> Clearly indicate the improvements that you make!!! You can for instance use titles like: <i>3.1. Improvement: Non-linear SVM with RBF Kernel.<i>\n",
    "</div>\n",
    "    \n",
    "---------------------------------------------------------------\n",
    "# 0. Data loading & Preprocessing\n",
    "\n",
    "## 0.1. Loading data\n",
    "The training set is many times smaller than the test set and this might strike you as odd, however, this is close to a real world scenario where your system might be put through daily use! In this session we will try to do the best we can with the data that we've got! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea7e082",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T21:22:20.635165Z",
     "iopub.status.busy": "2024-04-14T21:22:20.632511Z",
     "iopub.status.idle": "2024-04-14T21:22:44.975976Z",
     "shell.execute_reply": "2024-04-14T21:22:44.973748Z",
     "shell.execute_reply.started": "2024-04-14T21:22:20.635099Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2024-04-14T21:58:04.886420",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "\n",
    "train = pd.read_csv(\n",
    "    '/kaggle/input/kul-h02a5a-computer-vision-ga1-2024/train_set.csv', index_col = 0)\n",
    "train.index = train.index.rename('id')\n",
    "\n",
    "test = pd.read_csv(\n",
    "    '/kaggle/input/kul-h02a5a-computer-vision-ga1-2024/test_set.csv', index_col = 0)\n",
    "test.index = test.index.rename('id')\n",
    "\n",
    "# read the images as numpy arrays and store in \"img\" column\n",
    "train['img'] = [cv2.cvtColor(np.load('/kaggle/input/kul-h02a5a-computer-vision-ga1-2024/train/train_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n",
    "                for index, row in train.iterrows()]\n",
    "\n",
    "test['img'] = [cv2.cvtColor(np.load('/kaggle/input/kul-h02a5a-computer-vision-ga1-2024/test/test_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n",
    "                for index, row in test.iterrows()]\n",
    "  \n",
    "\n",
    "train_size, test_size = len(train),len(test)\n",
    "\n",
    "\"The training set contains {} examples, the test set contains {} examples.\".format(train_size, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d5691",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "*Note: this dataset is a subset of the* [*VGG face dataset*](https://www.robots.ox.ac.uk/~vgg/data/vgg_face/).\n",
    "\n",
    "## 0.2. A first look\n",
    "Let's have a look at the data columns and class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d97cb5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The training set contains an identifier, name, image information and class label\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e5a05",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The test set only contains an identifier and corresponding image information.\n",
    "\n",
    "test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3436d9ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T21:23:23.276319Z",
     "iopub.status.busy": "2024-04-14T21:23:23.275873Z",
     "iopub.status.idle": "2024-04-14T21:23:23.302040Z",
     "shell.execute_reply": "2024-04-14T21:23:23.300658Z",
     "shell.execute_reply.started": "2024-04-14T21:23:23.276285Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The class distribution in the training set:\n",
    "train.groupby('name').agg({'img':'count', 'class': 'max'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54395bfe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Note that **Jesse is assigned the classification label 1**, and **Mila is assigned the classification label 2**. The dataset also contains 20 images of **look alikes (assigned classification label 0)** and the raw images. \n",
    "\n",
    "## 0.3. Preprocess data\n",
    "### 0.3.1 Example: HAAR face detector\n",
    "In this example we use the [HAAR feature based cascade classifiers](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html) to detect faces, then the faces are resized so that they all have the same shape. If there are multiple faces in an image, we only take the first one. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> You can write temporary files to <code>/kaggle/temp/</code> or <code>../../tmp</code>, but they won't be saved outside of the current session\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a04ade",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T21:23:31.196770Z",
     "iopub.status.busy": "2024-04-14T21:23:31.196327Z",
     "iopub.status.idle": "2024-04-14T21:23:31.214457Z",
     "shell.execute_reply": "2024-04-14T21:23:31.213322Z",
     "shell.execute_reply.started": "2024-04-14T21:23:31.196730Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HAARPreprocessor():\n",
    "    \"\"\"Preprocessing pipeline built around HAAR feature based cascade classifiers. \"\"\"\n",
    "    \n",
    "    def __init__(self, path, face_size):\n",
    "        self.face_size = face_size\n",
    "        file_path = os.path.join(path, \"haarcascade_frontalface_default.xml\")\n",
    "        if not os.path.exists(file_path): \n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "            self.download_model(file_path)\n",
    "        \n",
    "        self.classifier = cv2.CascadeClassifier(file_path)\n",
    "  \n",
    "    def download_model(self, path):\n",
    "        url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/\"\\\n",
    "            \"haarcascades/haarcascade_frontalface_default.xml\"\n",
    "        \n",
    "        with request.urlopen(url) as r, open(path, 'wb') as f:\n",
    "            f.write(r.read())\n",
    "            \n",
    "    def detect_faces(self, img):\n",
    "        \"\"\"Detect all faces in an image.\"\"\"\n",
    "        \n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        return self.classifier.detectMultiScale(\n",
    "            img_gray,\n",
    "            scaleFactor=1.2,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30),\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "        \n",
    "    def extract_faces(self, img):\n",
    "        \"\"\"Returns all faces (cropped) in an image.\"\"\"\n",
    "        \n",
    "        faces = self.detect_faces(img)\n",
    "\n",
    "        return [img[y:y+h, x:x+w] for (x, y, w, h) in faces]\n",
    "    \n",
    "#     def preprocess(self, data_row):\n",
    "#         faces = self.extract_faces(data_row['img'])\n",
    "        \n",
    "#         # if no faces were found, return None\n",
    "#         if len(faces) == 0:\n",
    "#             nan_img = np.empty(self.face_size + (3,))\n",
    "#             nan_img[:] = np.nan\n",
    "#             return nan_img\n",
    "        \n",
    "#         # only return the first face\n",
    "#         return cv2.resize(faces[0], self.face_size, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    def preprocess(self, image):\n",
    "        faces = self.extract_faces(image)\n",
    "\n",
    "        # if no faces were found, return None\n",
    "        if len(faces) == 0:\n",
    "            nan_img = np.empty(self.face_size + (3,))\n",
    "            nan_img[:] = np.nan\n",
    "            return nan_img\n",
    "\n",
    "        face_list = [cv2.resize(face, self.face_size, interpolation = cv2.INTER_AREA) for face in faces]\n",
    "        # only return the second face if multiple faces available in the image\n",
    "        return list(face_list)\n",
    "            \n",
    "#     def __call__(self, data):\n",
    "#         return np.stack([self.preprocess(row) for _, row in data.iterrows()]).astype(int)\n",
    "\n",
    "def preprocess_data(data):\n",
    "    preprocessor = HAARPreprocessor(path='../../tmp', face_size=FACE_SIZE)\n",
    "    new_data = []\n",
    "    rows, _ = data.shape\n",
    "    for r in range(rows):\n",
    "        # print(r)\n",
    "        faces = preprocessor.preprocess(data.iloc[r]['img'])\n",
    "        num_faces = len(faces)\n",
    "        if num_faces == 0:\n",
    "            continue  # Skip this row if no faces are found\n",
    "        for i in range(num_faces):\n",
    "            new_row = data.iloc[r].copy()\n",
    "            new_row['img'] = faces[i]\n",
    "            new_data.append(new_row)\n",
    "    return pd.DataFrame(new_data)\n",
    "\n",
    "def remove_rows_with_nan_images(data):\n",
    "    # Create a mask to identify rows with NaN images\n",
    "    mask = data['img'].apply(lambda x: np.any(np.isnan(x)))\n",
    "\n",
    "    # Invert the mask to keep rows where img does not contain NaN\n",
    "    filtered_data = data[~mask]\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "def return_rows_with_nan_images(data):\n",
    "    # Create a mask to identify rows with NaN images\n",
    "    mask = data['img'].apply(lambda x: np.any(np.isnan(x)))\n",
    "\n",
    "    # Invert the mask to keep rows where img does not contain NaN\n",
    "    filtered_data = data[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541aa896",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Visualise**\n",
    "\n",
    "Let's plot a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1fb832",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T21:23:36.637967Z",
     "iopub.status.busy": "2024-04-14T21:23:36.637486Z",
     "iopub.status.idle": "2024-04-14T21:23:40.042138Z",
     "shell.execute_reply": "2024-04-14T21:23:40.040873Z",
     "shell.execute_reply.started": "2024-04-14T21:23:36.637929Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameter to play with \n",
    "# FACE_SIZE = (100, 100)\n",
    "FACE_SIZE = (250, 250)\n",
    "\n",
    "def plot_image_sequence(data, n, imgs_per_row=7):\n",
    "    n_rows = 1 + int(n/(imgs_per_row+1))\n",
    "    n_cols = min(imgs_per_row, n)\n",
    "\n",
    "    f,ax = plt.subplots(n_rows,n_cols, figsize=(10*n_cols,10*n_rows))\n",
    "    for i in range(n):\n",
    "        if n == 1:\n",
    "            ax.imshow(data[i])\n",
    "        elif n_rows > 1:\n",
    "            ax[int(i/imgs_per_row),int(i%imgs_per_row)].imshow(data[i])\n",
    "        else:\n",
    "            ax[int(i%n)].imshow(data[i])\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "#preprocessed data \n",
    "# preprocessor = HAARPreprocessor(path = '../../tmp', face_size=FACE_SIZE)\n",
    "\n",
    "train_faces = preprocess_data(train)\n",
    "train_faces_cleaned = remove_rows_with_nan_images(train_faces)\n",
    "train_faces_cleaned = train_faces_cleaned.reset_index(drop=False, inplace=False) # for plotting later to show misclassifications\n",
    "wrong_detections = [5, 18, 26, 28, 33, 35, 36, 37, 38, 39, 51, 54, 56, 58, 60, 62, 65, 70, 73, 76, 78, 85, 88, 89, 98, 99]\n",
    "\n",
    "# remove the rows with the level_0 value in the list wrong_detections\n",
    "train_faces_cleaned_mcr = train_faces_cleaned.drop(wrong_detections) # mcr - mis-classifications removed\n",
    "# replace row 34's name with \"Michael_Cera\" and Class with 0\n",
    "train_faces_cleaned_mcr.loc[49, 'name'] = \"Michael_Cera\"\n",
    "train_faces_cleaned_mcr.loc[49, 'class'] = 0\n",
    "\n",
    "# use train_faces_cleaned_mcr as the new train variable\n",
    "\n",
    "# train_X, train_y = preprocessor(train), train['class'].values\n",
    "# test_X = preprocessor(test)\n",
    "\n",
    "train_X, train_y = train_faces_cleaned_mcr['img'].values,  train_faces_cleaned_mcr['class'].values\n",
    "train_X = np.stack(train_faces_cleaned_mcr['img'], dtype=np.uint8)\n",
    "train_X = np.array([cv2.resize(x, (100,100), interpolation= cv2.INTER_LINEAR) for x in train_X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a42023c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Loading our own version of the test data after fixing it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb9b488",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T21:23:52.030499Z",
     "iopub.status.busy": "2024-04-14T21:23:52.029650Z",
     "iopub.status.idle": "2024-04-14T21:23:52.041205Z",
     "shell.execute_reply": "2024-04-14T21:23:52.039598Z",
     "shell.execute_reply.started": "2024-04-14T21:23:52.030464Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_convert_images_to_numpy(source_folder, target_folder, file_extension='.png'):\n",
    "    \"\"\"\n",
    "    Load images from a folder, convert them to numpy arrays, and save as .npy files in a target folder.\n",
    "\n",
    "    Args:\n",
    "        source_folder (str): Path to the folder containing the image files.\n",
    "        target_folder (str): Path to the folder where .npy files will be saved.\n",
    "        file_extension (str): The file extension of the images to be loaded.\n",
    "    \"\"\"\n",
    "    # Ensure the target directory exists\n",
    "    os.makedirs(target_folder, exist_ok=True)\n",
    "\n",
    "    # List all files in the source folder that match the extension\n",
    "    image_files = [f for f in os.listdir(source_folder) if f.endswith(file_extension)]\n",
    "    image_files.sort()\n",
    "\n",
    "    # Process each file\n",
    "    for file_name in image_files:\n",
    "        # Load the image using PIL\n",
    "        image_path = os.path.join(source_folder, file_name)\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Convert the image to a numpy array\n",
    "        image_array = np.array(image)\n",
    "\n",
    "        # Save the array to a file in the target folder\n",
    "        np.save(os.path.join(target_folder, file_name[:-len(file_extension)]), image_array)\n",
    "\n",
    "def load_numpy_images(folder_path, indices):\n",
    "    \"\"\"\n",
    "    Load .npy files from a folder and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing .npy files.\n",
    "        indices (list): A list of indices (as strings) which correspond to the filenames of .npy files to load.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of numpy arrays loaded from the specified files.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for index in indices:\n",
    "        file_path = os.path.join(folder_path, f\"{index}.npy\")\n",
    "        image = np.load(file_path, allow_pickle=False)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images.append(image)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c085e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "The modified dataset for testing is available at: https://www.kaggle.com/datasets/csap96/cropped-test/data?select=npy\n",
    "\n",
    "On the right side of this notebook you can click on \"Add Input\" and search for the following dataset: \"cropped_test_group13_KULCV2324\" before running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee7dbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T21:23:56.768059Z",
     "iopub.status.busy": "2024-04-14T21:23:56.767466Z",
     "iopub.status.idle": "2024-04-14T21:24:15.017309Z",
     "shell.execute_reply": "2024-04-14T21:24:15.016123Z",
     "shell.execute_reply.started": "2024-04-14T21:23:56.768015Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "# cropped_folder = '/kaggle/input/cropped-test/cropped'\n",
    "# numpy_folder = os.path.join(cropped_folder, 'npy')\n",
    "\n",
    "# # Convert images to numpy arrays and save them\n",
    "# load_and_convert_images_to_numpy(cropped_folder, numpy_folder)\n",
    "\n",
    "numpy_folder = '/kaggle/input/cropped-test/npy/npy'\n",
    "# Load and convert numpy files back to images\n",
    "indices = [str(n) for n in range(1816)]\n",
    "test_X_cropped = load_numpy_images(numpy_folder, indices) # this needs to be resized\n",
    "test_X = np.array([cv2.resize(x, (100,100), interpolation= cv2.INTER_LINEAR) for x in test_X_cropped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec2edd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T17:15:17.085895Z",
     "iopub.status.busy": "2024-04-14T17:15:17.085478Z",
     "iopub.status.idle": "2024-04-14T17:15:17.093499Z",
     "shell.execute_reply": "2024-04-14T17:15:17.092115Z",
     "shell.execute_reply.started": "2024-04-14T17:15:17.085864Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c303c91a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T17:15:20.479634Z",
     "iopub.status.busy": "2024-04-14T17:15:20.479208Z",
     "iopub.status.idle": "2024-04-14T17:15:24.645878Z",
     "shell.execute_reply": "2024-04-14T17:15:24.644522Z",
     "shell.execute_reply.started": "2024-04-14T17:15:20.479602Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot faces of Michael and Sarah\n",
    "\n",
    "plot_image_sequence(train_X[train_y == 0], n=20, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fbff36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T17:15:26.270872Z",
     "iopub.status.busy": "2024-04-14T17:15:26.270446Z",
     "iopub.status.idle": "2024-04-14T17:15:32.812843Z",
     "shell.execute_reply": "2024-04-14T17:15:32.811853Z",
     "shell.execute_reply.started": "2024-04-14T17:15:26.270841Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot faces of Jesse\n",
    "\n",
    "plot_image_sequence(train_X[train_y == 1], n=30, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bf6627",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot faces of Mila\n",
    "\n",
    "#plot_image_sequence(train_X[train_y == 2], n=30, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cf4e72",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 0.4. Store Preprocessed data (optional)\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\". Feel free to use this to store intermediary results.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2f8c7e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save preprocessed data\n",
    "# prep_path = '/kaggle/working/prepped_data/'\n",
    "# if not os.path.exists(prep_path):\n",
    "#     os.mkdir(prep_path)\n",
    "    \n",
    "# np.save(os.path.join(prep_path, 'train_X.npy'), train_X)\n",
    "# np.save(os.path.join(prep_path, 'train_y.npy'), train_y)\n",
    "# np.save(os.path.join(prep_path, 'test_X.npy'), test_X)\n",
    "\n",
    "# load preprocessed data\n",
    "# prep_path = '/kaggle/working/prepped_data/'\n",
    "# if not os.path.exists(prep_path):\n",
    "#     os.mkdir(prep_path)\n",
    "# train_X = np.load(os.path.join(prep_path, 'train_X.npy'))\n",
    "# train_y = np.load(os.path.join(prep_path, 'train_y.npy'))\n",
    "# test_X = np.load(os.path.join(prep_path, 'test_X.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731f197d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Now we are ready to rock!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3470ce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 0.5. Augmentations\n",
    "\n",
    "Need to update this further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c53b9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T17:18:10.892696Z",
     "iopub.status.busy": "2024-04-14T17:18:10.891892Z",
     "iopub.status.idle": "2024-04-14T17:18:15.825645Z",
     "shell.execute_reply": "2024-04-14T17:18:15.823542Z",
     "shell.execute_reply.started": "2024-04-14T17:18:10.892646Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_watermark(img, watermark_text=\"Watermark\", position=(5,\n",
    "                                                             33),\n",
    "                                                             opacity=0.2,\n",
    "                                                             font_scale=0.5,\n",
    "                                                             color=(255, 255, 255),\n",
    "                                                             background_color=(0, 0, 0)):\n",
    "\n",
    "    # Ensure img is a contiguous array of type uint8\n",
    "    img = np.ascontiguousarray(img).astype(np.uint8)\n",
    "\n",
    "    # Prepare overlay\n",
    "    overlay = img.copy()\n",
    "    overlay = np.ascontiguousarray(overlay).astype(np.uint8)\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    # Calculate text size\n",
    "    (text_width, text_height), _ = cv2.getTextSize(watermark_text, font, font_scale, thickness=2)\n",
    "    text_x, text_y = position\n",
    "\n",
    "    # Adjust rectangle position so it serves as a background\n",
    "    rect_start = (text_x, text_y - text_height - 5)\n",
    "    rect_end = (text_x + text_width, text_y + 10)\n",
    "\n",
    "    # Draw the rectangle\n",
    "    cv2.rectangle(overlay, rect_start, rect_end, background_color, thickness=cv2.FILLED)\n",
    "\n",
    "    # Draw the text on top\n",
    "    text_position = (text_x, text_y)\n",
    "    cv2.putText(overlay, watermark_text, text_position, font, font_scale, color, 2)\n",
    "\n",
    "    # Blend the overlay\n",
    "    cv2.addWeighted(overlay, opacity, img, 1 - opacity, 0, img)\n",
    "\n",
    "    return img\n",
    "\n",
    "def random_flip_rotate(image):\n",
    "    \"\"\"\n",
    "    Randomly flips and rotates the input image.\n",
    "\n",
    "    Args:\n",
    "        image: TensorFlow tensor representing the input image.\n",
    "\n",
    "    Returns:\n",
    "        Augmented image with random flips and rotations.\n",
    "    \"\"\"\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    # k = tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n",
    "    return image\n",
    "\n",
    "def color_jitter(image, brightness=0.1, contrast=0.3, saturation=0.2, hue=0.15):\n",
    "    \"\"\"\n",
    "    Randomly adjusts the brightness, contrast, saturation, and hue of an input image in integer format.\n",
    "    Converts image to float [0, 1], applies adjustments, and converts back to integer [0, 255].\n",
    "\n",
    "    Args:\n",
    "        image: TensorFlow tensor representing the input image in integer format [0, 255].\n",
    "        brightness: Maximum delta for brightness adjustment.\n",
    "        contrast: Maximum delta for contrast adjustment.\n",
    "        saturation: Maximum delta for saturation adjustment.\n",
    "        hue: Maximum delta for hue adjustment.\n",
    "\n",
    "    Returns:\n",
    "        Augmented image with color jittering in integer format [0, 255].\n",
    "    \"\"\"\n",
    "    # Convert image to float [0, 1]\n",
    "    image_float = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "    # Apply color jitter\n",
    "    image_float = tf.image.random_brightness(image_float, max_delta=brightness)\n",
    "    image_float = tf.image.random_contrast(image_float, lower=1-contrast, upper=1+contrast)\n",
    "    image_float = tf.image.random_saturation(image_float, lower=1-saturation, upper=1+saturation)\n",
    "    image_float = tf.image.random_hue(image_float, max_delta=hue)\n",
    "\n",
    "    # Clip the image data to ensure it's in the valid range\n",
    "    image_float = tf.clip_by_value(image_float, 0.0, 1.0)\n",
    "\n",
    "    # Convert back to integer format [0, 255]\n",
    "    aug_img = tf.cast(image_float * 255, tf.uint8)\n",
    "\n",
    "    return aug_img\n",
    "\n",
    "def add_noise(image, noise_level=0.05):\n",
    "    \"\"\"\n",
    "    Adds random noise to the input image. Assumes image is in [0, 255] integer format.\n",
    "\n",
    "    Args:\n",
    "        image: TensorFlow tensor representing the input image in [0, 255] integer format.\n",
    "        noise_level: Level of noise to be added, scaled for images in [0, 255] format.\n",
    "\n",
    "    Returns:\n",
    "        Augmented image with added noise, returned in the same format as the input.\n",
    "    \"\"\"\n",
    "    # Convert image to float [0, 1] for processing\n",
    "    image_float = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "    # Adjust noise level for [0, 1] scale\n",
    "    adjusted_noise_level = noise_level * 255.0\n",
    "\n",
    "    # Generate noise and add to the image\n",
    "    noise = tf.random.normal(shape=tf.shape(image_float), mean=0.0, stddev=adjusted_noise_level / 255.0)\n",
    "    noisy_image = tf.clip_by_value(image_float + noise, 0.0, 1.0)\n",
    "\n",
    "    # Convert back to [0, 255] integer format\n",
    "    noisy_image = tf.cast(noisy_image * 255, tf.uint8)\n",
    "\n",
    "    return noisy_image\n",
    "\n",
    "def random_erasing(image, sl=0.02, sh=0.4, r1=0.3):\n",
    "    \"\"\"\n",
    "    Randomly erases a rectangle region in the input image.\n",
    "\n",
    "    Args:\n",
    "        image: TensorFlow tensor representing the input image.\n",
    "        sl: Minimum proportion of erased area.\n",
    "        sh: Maximum proportion of erased area.\n",
    "        r1: Aspect ratio of the erased region.\n",
    "\n",
    "    Returns:\n",
    "        Augmented image with random erasing.\n",
    "    \"\"\"\n",
    "    image_shape = tf.shape(image)\n",
    "    h, w, _ = tf.unstack(image_shape)\n",
    "    area = tf.cast(w * h, tf.float32)\n",
    "\n",
    "    target_area = tf.random.uniform([], minval=sl, maxval=sh) * area\n",
    "    aspect_ratio = tf.random.uniform([], minval=r1, maxval=1/r1)\n",
    "\n",
    "    h_new = tf.cast(tf.round(tf.sqrt(target_area / aspect_ratio)), tf.int32)\n",
    "    w_new = tf.cast(tf.round(tf.sqrt(target_area * aspect_ratio)), tf.int32)\n",
    "\n",
    "    # Ensure the rectangle fits within the image\n",
    "    h_new = tf.minimum(h, h_new)\n",
    "    w_new = tf.minimum(w, w_new)\n",
    "\n",
    "    x1 = tf.random.uniform([], minval=0, maxval=(h - h_new + 1), dtype=tf.int32)\n",
    "    y1 = tf.random.uniform([], minval=0, maxval=(w - w_new + 1), dtype=tf.int32)\n",
    "\n",
    "    mask = tf.ones([h, w, 3], dtype=image.dtype)\n",
    "    erase_block = tf.zeros([h_new, w_new, 3], dtype=image.dtype)\n",
    "\n",
    "    # Calculate padding dimensions correctly\n",
    "    padding_top, padding_bottom = x1, (h - x1 - h_new)\n",
    "    padding_left, padding_right = y1, (w - y1 - w_new)\n",
    "\n",
    "    erase_block_padded = tf.pad(erase_block, [[padding_top, padding_bottom], [padding_left, padding_right], [0, 0]], constant_values=1)\n",
    "\n",
    "    # Apply the mask to the image\n",
    "    image = image * erase_block_padded\n",
    "    return image\n",
    "\n",
    "\n",
    "# Function to apply Gaussian blur on an image using OpenCV\n",
    "def gaussian_blur(image, blur_intensity=5):\n",
    "    # Apply Gaussian Blur\n",
    "    blurred_image = cv2.GaussianBlur(image, (blur_intensity, blur_intensity), 0)\n",
    "    return blurred_image\n",
    "\n",
    "def sharpen_image(image):\n",
    "    \"\"\"\n",
    "    Sharpens an image using a specific sharpening kernel.\n",
    "    \n",
    "    Args:\n",
    "    image (numpy.ndarray): The input image as a numpy array.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The sharpened image.\n",
    "    \"\"\"\n",
    "    # Define the sharpening kernel, the numbers can be adjusted to change the effect\n",
    "    sharpening_kernel = np.array([[-1, -1, -1],\n",
    "                                  [-1,  9, -1],\n",
    "                                  [-1, -1, -1]])\n",
    "    \n",
    "    # Apply the sharpening kernel to the input image\n",
    "    sharpened_image = cv2.filter2D(image, -1, sharpening_kernel)\n",
    "    \n",
    "    return sharpened_image\n",
    "\n",
    "def pixelate_image(image, pixel_size=3):\n",
    "    \"\"\"\n",
    "    Pixelates an image by reducing its resolution and then resizing it back to its original size.\n",
    "    \n",
    "    Args:\n",
    "    image (numpy.ndarray): The input image as a numpy array.\n",
    "    pixel_size (int): The size of each block of pixels in the pixelated image.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The pixelated image.\n",
    "    \"\"\"\n",
    "    # Check the input image's dimensions\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Resize the image to a smaller size\n",
    "    small_image = cv2.resize(image, (width // pixel_size, height // pixel_size), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Scale it back up to the original size\n",
    "    pixelated_image = cv2.resize(small_image, (width, height), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    return pixelated_image\n",
    "\n",
    "def apply_augmentations(train_X, train_y):\n",
    "    \"\"\"\n",
    "    Applies augmentations to the training set.\n",
    "\n",
    "    Args:\n",
    "        train_X: Training set features.\n",
    "        train_y: Training set labels.\n",
    "\n",
    "    Returns:\n",
    "        Augmented training set features and labels.\n",
    "    \"\"\"\n",
    "    augmentations = [random_flip_rotate,\n",
    "                     color_jitter,\n",
    "                     add_noise,\n",
    "                     random_erasing,\n",
    "                     add_watermark,\n",
    "                     gaussian_blur,\n",
    "                     sharpen_image,\n",
    "                     pixelate_image]\n",
    "    new_train_X = []\n",
    "    new_train_y = []\n",
    "    for i in range(len(train_X)):\n",
    "\n",
    "        image = train_X[i]\n",
    "        # add the original image\n",
    "        new_train_X.append(image)\n",
    "        new_train_y.append(train_y[i])\n",
    "\n",
    "        for augmentation in augmentations:\n",
    "\n",
    "            aug_image = augmentation(image)\n",
    "            # add the augmented image\n",
    "            new_train_X.append(aug_image)\n",
    "            new_train_y.append(train_y[i])\n",
    "    return np.array(new_train_X), np.array(new_train_y)\n",
    "\n",
    "temp_x = list(train_X)\n",
    "temp_y = train_y\n",
    "\n",
    "for x in [65,35,28,24,23,14]:\n",
    "    temp_x.pop(x)\n",
    "    temp_y = np.delete(temp_y, x)\n",
    "temp_x = np.array(temp_x)\n",
    "\n",
    "new_train_X, new_train_y = apply_augmentations(temp_x, temp_y)\n",
    "new_train_X.shape, new_train_y.shape\n",
    "\n",
    "plot_image_sequence(new_train_X, n=20, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48187b0d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 1. Feature Representations\n",
    "## 1.0. Example: Identify feature extractor\n",
    "Our example feature extractor doesn't actually do anything... It just returns the input:\n",
    "$$\n",
    "\\forall x : f(x) = x.\n",
    "$$\n",
    "\n",
    "It does make for a good placeholder and baseclass ;)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0d5ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T21:24:59.114478Z",
     "iopub.status.busy": "2024-04-14T21:24:59.113662Z",
     "iopub.status.idle": "2024-04-14T21:24:59.122193Z",
     "shell.execute_reply": "2024-04-14T21:24:59.120979Z",
     "shell.execute_reply.started": "2024-04-14T21:24:59.114443Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IdentityFeatureExtractor:\n",
    "    \"\"\"A simple function that returns the input\"\"\"\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "    def convertToGray(self,X):\n",
    "        grayImages = np.empty((np.shape(X)[0],np.shape(X)[1],np.shape(X)[2]))\n",
    "        for index,image in enumerate(X):\n",
    "          img_float32 = np.float32(image)\n",
    "          gray = cv2.cvtColor(img_float32, cv2.COLOR_BGR2GRAY)\n",
    "          grayImages[index] = gray\n",
    "        return grayImages\n",
    "\n",
    "    def rotate(self,X):\n",
    "      rotatedImages = []\n",
    "      for image in X:\n",
    "        print(X)\n",
    "        rotated = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
    "        rotatedImages.append(rotated)\n",
    "      return rotatedImages\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e72fd7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1.1. Baseline 1: HOG feature extractor/Scale Invariant Feature Transform\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f9852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T17:18:28.086569Z",
     "iopub.status.busy": "2024-04-14T17:18:28.086120Z",
     "iopub.status.idle": "2024-04-14T17:18:28.095730Z",
     "shell.execute_reply": "2024-04-14T17:18:28.094460Z",
     "shell.execute_reply.started": "2024-04-14T17:18:28.086537Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HOGFeatureExtractor(IdentityFeatureExtractor):\n",
    "    \"\"\"TODO: this feature extractor is under construction\"\"\"\n",
    "    # documentation to refer to: https://github.com/scikit-image/scikit-image/blob/main/skimage/feature/_hog.py\n",
    "    def __init__(self, **params):\n",
    "        self.params = params\n",
    "\n",
    "    def extract_hog_features(self, X,\n",
    "                             orientations = 9,\n",
    "                             ppc = 2,\n",
    "                             cpb = 1,\n",
    "                             visualise = True,\n",
    "                             block_norm='L2-Hys',\n",
    "                             transform_sqrt=True,\n",
    "                             feature_vector=True,\n",
    "                             channel_axis=2):\n",
    "\n",
    "      feature_vector, hog_image = hog(X,\n",
    "                                      orientations=orientations,\n",
    "                                      pixels_per_cell=(ppc, ppc),\n",
    "                                      cells_per_block=(cpb, cpb),\n",
    "                                      visualize=visualise,\n",
    "                                      block_norm=block_norm,\n",
    "                                      transform_sqrt=transform_sqrt,\n",
    "                                      feature_vector=feature_vector,\n",
    "                                      channel_axis=channel_axis)\n",
    "      return feature_vector, hog_image\n",
    "\n",
    "    def transform(self, X):\n",
    "        features, hogimage = self.extract_hog_features(X)\n",
    "        return features, hogimage \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62af399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T17:19:27.627859Z",
     "iopub.status.busy": "2024-04-14T17:19:27.627420Z",
     "iopub.status.idle": "2024-04-14T17:19:28.990736Z",
     "shell.execute_reply": "2024-04-14T17:19:28.989387Z",
     "shell.execute_reply.started": "2024-04-14T17:19:27.627826Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hogfe = HOGFeatureExtractor()\n",
    "feature_vector, image = hogfe.transform(new_train_X[0])\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4faaf2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**SIFT**\n",
    "\n",
    "Scale Invariant Feature Transform is a robust means of identifying areas of interest from an image using Difference of Gaussian blurs and then assigning a signature to those areas of interest with a local gradient of histograms. It is commonly used for performing stitching and correspondance between two images, or for object detection in images which could be incorporated into a query and retrieval system.\n",
    "\n",
    "Below is the implementation for extracting and describing key features of an image, as well as performing nearest matches between two images and clustering continuous features into a Bag of Words discrete histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fdfec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T17:19:38.012425Z",
     "iopub.status.busy": "2024-04-14T17:19:38.011981Z",
     "iopub.status.idle": "2024-04-14T17:19:38.039479Z",
     "shell.execute_reply": "2024-04-14T17:19:38.038003Z",
     "shell.execute_reply.started": "2024-04-14T17:19:38.012395Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SIFT_FeatureExtractor(IdentityFeatureExtractor):\n",
    "\n",
    "  def __init__(self,**params):\n",
    "    self.params = params\n",
    "    self.words = None\n",
    "\n",
    "  # This transform method should return a list of feature descriptors from the entire dataset X\n",
    "  def transform(self,X):\n",
    "    all_descriptors = []\n",
    "    all_keypoints = []\n",
    "    sift = cv2.SIFT_create(nOctaveLayers = self.params[\"Octaves\"],\n",
    "                          nfeatures =  self.params[\"NumberOfFeatures\"],\n",
    "                          contrastThreshold = self.params[\"ConstrastThreshold\"],\n",
    "                          edgeThreshold = self.params[\"EdgeThreshold\"],\n",
    "                          sigma = self.params[\"Smoothing\"] )\n",
    "    for img in X:\n",
    "        gray = cv2.cvtColor(img.astype(dtype=np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "        kp, des = sift.detectAndCompute(gray, None)\n",
    "        all_descriptors.append(des)\n",
    "        all_keypoints.append(kp)\n",
    "    return all_keypoints, all_descriptors\n",
    "\n",
    "  # This method returns the keypoints and corresponding descriptors for ONE image\n",
    "  def transformOne(self,X,index):\n",
    "    sift = cv2.SIFT_create(nfeatures =  self.params[\"NumberOfFeatures\"],\n",
    "                          contrastThreshold = self.params[\"ConstrastThreshold\"],\n",
    "                          edgeThreshold = self.params[\"EdgeThreshold\"],\n",
    "                          sigma = self.params[\"Smoothing\"] )\n",
    "    gray = cv2.cvtColor(X[index].astype(dtype=np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "    kp, des = sift.detectAndCompute(gray, None)\n",
    "    return kp, des\n",
    "\n",
    "  # This method finds and plots points of interest on ONE image\n",
    "  def showFeatures(self,X,index):\n",
    "    kp, des = self.transformOne(X,index)\n",
    "    output_image = cv2.drawKeypoints(X[index], kp, 0, (0, 255, 0),\n",
    "                                 flags=cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
    "    print(\"{} features detected\".format(np.shape(des)[0]))\n",
    "    plt.imshow(output_image)\n",
    "    plt.show()\n",
    "\n",
    "  # This method finds and plots matches between two images in the dataset\n",
    "  def match(self,X,index1,index2):\n",
    "    kp1,des1 = self.transformOne(X,index1)\n",
    "    kp2,des2 = self.transformOne(X,index2)\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck = True)\n",
    "    matches = bf.match(des1,des2)\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "    img_matches = cv2.drawMatches(X[index1], kp1, X[index2], kp2, matches, outImg = np.empty((1,1)))\n",
    "    plt.imshow(img_matches)\n",
    "    plt.show()\n",
    "\n",
    "  # This method gathers descriptors of different lengths into a single array\n",
    "  def _gather_descriptors(self,descriptors_list):\n",
    "    descriptors = np.concatenate(descriptors_list, axis=0)\n",
    "    return descriptors\n",
    "\n",
    "  # This method initialises a set of K clusters representing different words\n",
    "  def createKWords(self,descriptors,k):\n",
    "    descriptors = self._gather_descriptors(descriptors)\n",
    "    kmeans = KMeans(n_clusters=k,random_state=42)\n",
    "    kmeans.fit(descriptors)\n",
    "    self.words = kmeans\n",
    "    print(\"{} words initialised from fed descriptors\".format(k))\n",
    "\n",
    "  # This method returns a set of words from a set of descriptors\n",
    "  def getWords(self,descriptors):\n",
    "    X_words = []\n",
    "    for des in descriptors:\n",
    "      features = np.array([0] * np.shape(self.words.cluster_centers_)[0])\n",
    "      distance = cdist(des, self.words.cluster_centers_, metric='euclidean')\n",
    "      argmin = np.argmin(distance, axis = 1)\n",
    "      for j in argmin:\n",
    "        features[j] += 1\n",
    "      X_words.append(features)\n",
    "    return X_words\n",
    "\n",
    "  def __call__(self, X):\n",
    "    return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddfe283",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "With SIFT, we can extract key features and describe them with a histogram of gradients. Furthermore, we can perform matching of features between images for tasks such as stitching and correspondance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3bd6a0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters chosen from:\n",
    "# https://www.researchgate.net/publication/366445603_A_hybrid_approach_for_face_recognition_using_a_convolutional_neural_network_combined_with_feature_extraction_techniques\n",
    "\n",
    "PARAMS = {\n",
    "    \"Octaves\":3,\n",
    "    \"NumberOfFeatures\": 80,\n",
    "    \"ConstrastThreshold\": 0.04,\n",
    "    \"EdgeThreshold\": 10,\n",
    "    \"Smoothing\": 1.6,\n",
    "}\n",
    "\n",
    "trans = SIFT_FeatureExtractor(**PARAMS)\n",
    "all_keypoints, all_features = trans(new_train_X) #Note that these are LISTS with a length equal to the training set size\n",
    "trans.showFeatures(new_train_X,0)\n",
    "trans.match(new_train_X,0,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d44ee8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "To convert a set of continuous descriptors into discrete features, we can use Bag of Words representation. This will cluster the descriptors into a set of distinct features which describe the image. This signature is invariant to spatial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9967fe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "numClusters = 20\n",
    "\n",
    "trans.createKWords(all_features,numClusters)\n",
    "all_words = trans.getWords(all_features)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(len(all_words[0])),all_words[0])\n",
    "plt.title('Bag of Words descriptor')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b998b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We can compare how similar two Bag of Words descriptors are via cosine similarity, which compares relative distribution of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a5570d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_word_descriptors(words1,words2):\n",
    "  return 1-cosine(words1,words2)\n",
    "\n",
    "def test_cosine(stacked_vectors,train_y,test_index):\n",
    "  cosine_similarities = []\n",
    "  top_k = int(6)\n",
    "  for sample in range(np.shape(stacked_vectors)[0]):\n",
    "    cosine_similarity = 1 - cosine(stacked_vectors[test_index], stacked_vectors[sample])\n",
    "    cosine_similarities.append(cosine_similarity)\n",
    "  cosine_similarities = np.array(cosine_similarities)\n",
    "  ind = np.argpartition(cosine_similarities, -1*top_k)[-1*top_k:]\n",
    "  return train_y[test_index], mode(train_y[ind[:-1]]).mode, train_y[test_index] == mode(train_y[ind[:-1]]).mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ae2464",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "index_1 = 0\n",
    "index_2 = 3\n",
    "index_3 = 7\n",
    "plt.imshow(new_train_X[index_1])\n",
    "plt.show()\n",
    "plt.imshow(new_train_X[index_2])\n",
    "plt.show()\n",
    "plt.imshow(new_train_X[index_3])\n",
    "plt.show()\n",
    "\n",
    "cosine_similarity1 = compare_word_descriptors(all_words[index_1],all_words[index_2])\n",
    "cosine_similarity2 = compare_word_descriptors(all_words[index_1],all_words[index_3])\n",
    "\n",
    "\n",
    "print(\"Similarity between visual description of images {} and {} = {}\".format(index_1,index_2,cosine_similarity1))\n",
    "print(\"Similarity between visual description of images {} and {} = {}\".format(index_1,index_3,cosine_similarity2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9414e3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We can even determine how discriminative our choice of hyperparameters is on the training set by performing a nearest neighbour search on the word cluster centres for each training image. From this we can calculate accuracies and an F1 score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc188d2b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NA_result = []\n",
    "J_result = []\n",
    "M_result = []\n",
    "\n",
    "targets = []\n",
    "preds = []\n",
    "for i in range(np.shape(all_words)[0]):\n",
    "  target, pred, result = test_cosine(all_words,new_train_y,i)\n",
    "  targets.append(target)\n",
    "  preds.append(pred)\n",
    "  if target == 0:\n",
    "    NA_result.append(result)\n",
    "  elif target == 1:\n",
    "    J_result.append(result)\n",
    "  elif target == 2:\n",
    "    M_result.append(result)\n",
    "total_result = NA_result+J_result+M_result\n",
    "# Calculate precision, recall, and F1-score for each class\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(np.array(targets), np.array(preds), average=None)\n",
    "# Calculate the average F1-score across all classes\n",
    "average_f1_score = sum(f1_score) / len(f1_score)\n",
    "print(\"Total accuracy = \", total_result.count(True) / (total_result.count(True)+total_result.count(False)))\n",
    "print(\"NA accuracy = \", NA_result.count(True) / (NA_result.count(True)+NA_result.count(False)))\n",
    "print(\"J accuracy = \", J_result.count(True) / (J_result.count(True)+J_result.count(False)))\n",
    "print(\"M accuracy = \", M_result.count(True) / (M_result.count(True)+M_result.count(False)))\n",
    "print(\"F1 score = \", average_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a5b077",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Furthermore, we can perform a hyperparameter gridsearch to determine a best set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b37f31",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "\n",
    "for octave in np.arange(3,6,1):\n",
    "  for contrast in np.arange(0.01,0.07,0.02):\n",
    "    for edge in np.arange(10,30,10):\n",
    "      for smoothing in np.arange(1.0,4.0,1.0):\n",
    "        for k in np.arange(20,50,10):\n",
    "\n",
    "            PARAMS = {\n",
    "                    \"NumberOfFeatures\": 100,\n",
    "                    \"Octaves\": octave,\n",
    "                    \"ConstrastThreshold\": contrast,\n",
    "                    \"EdgeThreshold\": edge,\n",
    "                    \"Smoothing\": smoothing,\n",
    "                      }\n",
    "\n",
    "            trans = SIFT_FeatureExtractor(**PARAMS)\n",
    "            all_keypoints, all_features = trans(train_X)\n",
    "            trans.showFeatures(train_X,0)\n",
    "            trans.createKWords(all_features,k)\n",
    "            all_words = trans.getWords(all_features)\n",
    "\n",
    "            NA_result = []\n",
    "            J_result = []\n",
    "            M_result = []\n",
    "            targets = []\n",
    "            preds = []\n",
    "            for i in range(np.shape(all_words)[0]):\n",
    "              target, pred, result = test_cosine(all_words,train_y,i)\n",
    "              targets.append(target)\n",
    "              preds.append(pred)\n",
    "              if target == 0:\n",
    "                NA_result.append(result)\n",
    "              elif target == 1:\n",
    "                J_result.append(result)\n",
    "              elif target == 2:\n",
    "                M_result.append(result)\n",
    "            total_result = NA_result+J_result+M_result\n",
    "            # Calculate precision, recall, and F1-score for each class\n",
    "            precision, recall, f1_score, _ = precision_recall_fscore_support(np.array(targets), np.array(preds), average=None)\n",
    "            # Calculate the average F1-score across all classes\n",
    "            average_f1_score = sum(f1_score) / len(f1_score)\n",
    "            print(\"Octaves = \", octave, \" ContrastThreshold = \", contrast, \" EdgeThreshold = \", edge, \" Smoothing = \", smoothing, \" K = \", k)\n",
    "            print(\"F1 score = \", average_f1_score)\n",
    "\n",
    "            results_dict[(octave,contrast,edge,smoothing,k)] = average_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d66ac5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "highest_keys = sorted(results_dict, key=lambda x: results_dict[x], reverse=True)[:10]\n",
    "\n",
    "print(\"Keys of the 5 highest float values:\", highest_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b597f3d6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 1.1.1. t-SNE Plots\n",
    "\n",
    "T-distributed stochastic neighbour embedding is a means of projecting data in a high dimensional space to a lower space. This improves visualisation of clusters and relations between observations. We can perform this on the K-dimensional visual words descriptors to indicate a level of seperability between the classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e8939",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "PARAMS = {\n",
    "    \"Octaves\":4,\n",
    "    \"NumberOfFeatures\": 100,\n",
    "    \"ConstrastThreshold\": 0.01,\n",
    "    \"EdgeThreshold\": 30,\n",
    "    \"Smoothing\": 2.0,\n",
    "}\n",
    "\n",
    "trans = SIFT_FeatureExtractor(**PARAMS)\n",
    "all_keypoints, all_features = trans(train_X) #Note that these are LISTS with a length equal to the training set size\n",
    "numClusters = 70\n",
    "trans.createKWords(all_features,numClusters)\n",
    "all_words = trans.getWords(all_features)\n",
    "\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=3).fit_transform(np.array(all_words))\n",
    "\n",
    "color_map = {\n",
    "    0: 'r',   # Red\n",
    "    1: 'g',   # Green\n",
    "    2: 'b',   # Blue\n",
    "}\n",
    "\n",
    "color_array = []\n",
    "for i in range(train_y.shape[0]):\n",
    "      color_array.append(color_map[train_y[i]])\n",
    "\n",
    "print(\"Jesse: Green, Mila: Blue, Michael or Sarah: Red\")\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_embedded[:,0],X_embedded[:,1],c=color_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1de5e5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Given the sparsity of the different classes, it is hard to make out distinctive clusters. Hence SIFT features from the unaltered dataset might not be a good medium for performing classification.\n",
    "\n",
    "Using data augmentation to enrich the training set could be an avenue for improving the discriminator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eae386f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"Octaves\":4,\n",
    "    \"NumberOfFeatures\": 100,\n",
    "    \"ConstrastThreshold\": 0.01,\n",
    "    \"EdgeThreshold\": 30,\n",
    "    \"Smoothing\": 2.0,\n",
    "}\n",
    "\n",
    "trans = SIFT_FeatureExtractor(**PARAMS)\n",
    "all_keypoints, all_features = trans(new_train_X) #Note that these are LISTS with a length equal to the training set size\n",
    "numClusters = 70\n",
    "trans.createKWords(all_features,numClusters)\n",
    "all_words = trans.getWords(all_features)\n",
    "\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=3).fit_transform(np.array(all_words))\n",
    "\n",
    "color_map = {\n",
    "    0: 'r',   # Red\n",
    "    1: 'g',   # Green\n",
    "    2: 'b',   # Blue\n",
    "}\n",
    "\n",
    "color_array = []\n",
    "for i in range(new_train_y.shape[0]):\n",
    "      color_array.append(color_map[new_train_y[i]])\n",
    "\n",
    "print(\"Jesse: Green, Mila: Blue, Michael or Sarah: Red\")\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_embedded[:,0],X_embedded[:,1],c=color_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e75206a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "From the t-SNE plot of the augmented dataset, we can see that barring a handful of outliers the blue and green dots are fairly seperable.\n",
    "This suggests that SIFT features enables some good discrimnation between the pictures of Mila Kunis and Jesse Eisenberg.\n",
    "However, the random positions of the red dots suggest that the added images of Michael Cera and Sarah Nyland confuses the classification. This can be particularly seen by how using a nearest neighbour method via cosine similarity will often misclassify a picture of Mila Kunis as Sarah Nyland."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd926d59",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 1.1.2. Discussion\n",
    "\n",
    "Using SIFT, we have demonstrated that an image can be characterised by its local feature descriptors. This representation is far more robust and information enriched than global characterisation methods for matching such as template matching using cross-correlation between images.\n",
    "\n",
    "The challenge with using SIFT is converting extracted histograms of gradients into a reliable feature space to be used by a classifier. This is addressed by employing K-means clustering to form a Bag of Visual words representation.\n",
    "\n",
    "Another challenge that we have demonstrated is the choice of hyperparameters for blob detection to find key areas of interest. For different tasks, details at different scales are much more informative than others. For example, discriminating for a more simple task such as between general images of men and women is more dependent on large details whereas for discrimnation between similar looking men and women requires a focus on smaller details.\n",
    "\n",
    "For the case of discriminating between Jesse Eisenberg and Mila Kunis, SIFT features are shown to be a good choice for a discriminative features space.\n",
    "However, with the inclusion of similar looking people the task becomes much harder since features across different scales become much more important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb00dc5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1.2. Baseline 2: PCA feature extractor\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e36b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T21:30:51.243347Z",
     "iopub.status.busy": "2024-04-14T21:30:51.242856Z",
     "iopub.status.idle": "2024-04-14T21:30:51.312890Z",
     "shell.execute_reply": "2024-04-14T21:30:51.311310Z",
     "shell.execute_reply.started": "2024-04-14T21:30:51.243313Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PCA_AllColourscales_FeatureExtractor(IdentityFeatureExtractor):\n",
    "    def __init__(self, train, labels, n_components, size, colour):\n",
    "      self.n_components = n_components\n",
    "      self.size = size\n",
    "      self.train = train\n",
    "      self.labels = labels\n",
    "      self.colour=colour\n",
    "      self.dim_reduction()\n",
    "\n",
    "\n",
    "    # Perform dimensionality reduction\n",
    "    def dim_reduction(self):\n",
    "      # If working in greyscale...\n",
    "      if self.colour == False:\n",
    "        gray_imgs = np.zeros((self.train.shape[0], self.train.shape[1], self.train.shape[2]), dtype=np.int32)\n",
    "        for img in range(self.train.shape[0]):\n",
    "          gray_imgs[img,:,:] = cv2.cvtColor(np.squeeze(self.train[img,:,:,:]).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "        self.gray_eigenfaces, self.gray_features, self.gray_mean_vec, self.gray_eigenvalues = self.rest_reduct(gray_imgs)\n",
    "      \n",
    "      # If working in colourscale...\n",
    "      elif self.colour == True:\n",
    "        blue_img = np.zeros((self.train.shape[0], self.train.shape[1], self.train.shape[2]), dtype=np.int32)\n",
    "        red_img = np.zeros((self.train.shape[0], self.train.shape[1], self.train.shape[2]), dtype=np.int32)\n",
    "        green_img = np.zeros((self.train.shape[0], self.train.shape[1], self.train.shape[2]), dtype=np.int32)\n",
    "        for img in range(self.train.shape[0]):\n",
    "          # Need to split the images into separate colour channels\n",
    "          B, G, R = cv2.split(self.train[img,:,:,:])\n",
    "          blue_img[img, :, :] = B\n",
    "          red_img[img, :, :] = R\n",
    "          green_img[img, :, :] = G\n",
    "\n",
    "        self.blue_eigenfaces, self.blue_features, self.blue_mean_vec, self.blue_eigenvalues = self.rest_reduct(blue_img)\n",
    "        self.red_eigenfaces, self.red_features, self.red_mean_vec, self.red_eigenvalues = self.rest_reduct(red_img)\n",
    "        self.green_eigenfaces, self.green_features, self.green_mean_vec, self.green_eigenvalues = self.rest_reduct(green_img)\n",
    "      return self\n",
    "\n",
    "\n",
    "    # More of the dimensionality reduction method\n",
    "    def rest_reduct(self, imgstack):\n",
    "      vec = imgstack.reshape((self.train.shape[0],-1))\n",
    "      # Obtain vector of mean values for data normalization / centering\n",
    "      mean_vec = np.mean(vec, 0)\n",
    "      vec_mean_subtracted = vec - mean_vec\n",
    "      # Obtain covariance matrix of centred values\n",
    "      cov_matrix = np.dot(vec_mean_subtracted, vec_mean_subtracted.T)/self.train.shape[0]\n",
    "      eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "      # Sort eigenvectors by ascending eigenvalues\n",
    "      sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "      eigenvalues = eigenvalues[sorted_indices]\n",
    "      eigenvectors = eigenvectors[:, sorted_indices]\n",
    "      sum = np.sum(eigenvalues)\n",
    "      sum_95 = self.n_components/100 * sum\n",
    "      temp=0\n",
    "      p=0\n",
    "      while temp < sum_95:\n",
    "        temp += eigenvalues[p]\n",
    "        p += 1\n",
    "      eigenfaces = np.dot(vec_mean_subtracted.T, eigenvectors)\n",
    "      eigenfaces= eigenfaces / np.linalg.norm(eigenfaces, axis=0)\n",
    "      eigenfaces = eigenfaces[:,:p]\n",
    "      features = np.dot(vec_mean_subtracted, eigenfaces)\n",
    "      return eigenfaces, features, mean_vec, eigenvalues\n",
    "\n",
    "    # This method controls getting the appropriate eigenfaces according to the colourscale being used\n",
    "    def get_eigenfaces(self):\n",
    "      if self.colour == False:\n",
    "        return self.gray_eigenfaces\n",
    "      else:\n",
    "        return self.red_eigenfaces, self.green_eigenfaces, self.blue_eigenfaces\n",
    "\n",
    "    # This method controls getting the appropriate features according to the colourscale being used\n",
    "    def get_features(self):\n",
    "      if self.colour == False:\n",
    "        return self.gray_features\n",
    "      else:\n",
    "        return self.red_features, self.green_features, self.blue_features\n",
    "    \n",
    "    # Transform input data to existing eigenvectors\n",
    "    def transform(self,X):\n",
    "      #if grey\n",
    "      if self.colour == False:\n",
    "        gray_img = np.zeros((X.shape[0], X.shape[1], X.shape[2]), dtype=np.int32)\n",
    "        for img in range(X.shape[0]):\n",
    "            gray_img[img,:,:] = cv2.cvtColor(np.squeeze(X[img,:,:,:]).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "        gray_vec = gray_img.reshape((X.shape[0],-1))\n",
    "        vec_mean_subtracted = gray_vec - self.gray_mean_vec\n",
    "        features = np.dot(vec_mean_subtracted, self.eigenfaces)\n",
    "        return features\n",
    "      #if colour\n",
    "      elif self.colour == True:\n",
    "        blue_img = np.zeros((self.train.shape[0], self.train.shape[1], self.train.shape[2]), dtype=np.int32)\n",
    "        red_img = np.zeros((self.train.shape[0], self.train.shape[1], self.train.shape[2]), dtype=np.int32)\n",
    "        green_img = np.zeros((self.train.shape[0], self.train.shape[1], self.train.shape[2]), dtype=np.int32)\n",
    "\n",
    "        for img in range(X.shape[0]):\n",
    "          B, G, R = cv2.split(self.X[img,:,:,:])\n",
    "          blue_img[img, :, :] = B\n",
    "          red_img[img, :, :] = R\n",
    "          green_img[img, :, :] = G\n",
    "        red_vec = red_img.reshape((X.shape[0], -1))\n",
    "        Rvec_mean_subtracted = red_vec - self.red_mean_vec\n",
    "        Rfeatures = np.dot(Rvec_mean_subtracted, self.red_eigenfaces)\n",
    "\n",
    "        blue_vec = blue_img.reshape((X.shape[0], -1))\n",
    "        Bvec_mean_subtracted = blue_vec - self.blue_mean_vec\n",
    "        Bfeatures = np.dot(Bvec_mean_subtracted, self.blue_eigenfaces)\n",
    "\n",
    "        green_vec = green_img.reshape((X.shape[0], -1))\n",
    "        Gvec_mean_subtracted = green_vec - self.green_mean_vec\n",
    "        Gfeatures = np.dot(Gvec_mean_subtracted, self.green_eigenfaces)\n",
    "        return Rfeatures, Gfeatures, Bfeatures\n",
    "\n",
    "    def inverse_transform(self, X, n_eigenfaces_vec):\n",
    "      if self.colour == False:\n",
    "        if n_eigenfaces_vec == 0:\n",
    "            gray_img =  self.gray_mean_vec.reshape(X.shape[0], self.size[0], self.size[1])\n",
    "            return gray_img\n",
    "        else:\n",
    "          rec_img_vec = np.dot(X[:,:n_eigenfaces_vec], self.gray_eigenfaces.T[:n_eigenfaces_vec,:])\n",
    "          rec_img_vec += self.gray_mean_vec\n",
    "          rec_gray_img = rec_img_vec.reshape(X.shape[0], self.size[0], self.size[1])\n",
    "          return rec_gray_img\n",
    "      else:\n",
    "        reconstructed_red = self.inv_method(X[0], n_eigenfaces_vec[0], 'red')\n",
    "        reconstructed_green = self.inv_method(X[1], n_eigenfaces_vec[1], 'green')\n",
    "        reconstructed_blue = self.inv_method(X[2], n_eigenfaces_vec[2], 'blue')\n",
    "        if n_eigenfaces_vec[0] == 0:\n",
    "          return cv2.merge([reconstructed_blue, reconstructed_green, reconstructed_red])\n",
    "        else:\n",
    "          total_images = np.zeros((reconstructed_red.shape[0], reconstructed_red.shape[1], reconstructed_red.shape[2], 3), dtype=np.int32)\n",
    "          for i in range(reconstructed_red.shape[0]):\n",
    "            total_images[i, :, :] = cv2.merge([reconstructed_blue[i], reconstructed_green[i], reconstructed_red[i]])\n",
    "          return total_images\n",
    "\n",
    "\n",
    "    def inv_method(self, X, n_eigenfaces, colour):\n",
    "      if n_eigenfaces == 0:\n",
    "        if colour == 'gray' or colour == 'grey':\n",
    "          reconstructed =  self.gray_mean_vec.reshape(self.size[0], self.size[1])\n",
    "        elif colour == 'red':\n",
    "          reconstructed =  self.red_mean_vec.reshape(self.size[0], self.size[1])\n",
    "        elif colour == 'green':\n",
    "          reconstructed =  self.green_mean_vec.reshape(self.size[0], self.size[1])\n",
    "        elif colour == 'blue':\n",
    "          reconstructed =  self.blue_mean_vec.reshape(self.size[0], self.size[1])\n",
    "        return reconstructed\n",
    "      else:\n",
    "        if colour == 'gray' or colour == 'grey':\n",
    "          rec_img_vec = np.dot(X[:,:n_eigenfaces], self.gray_eigenfaces.T[:n_eigenfaces,:])\n",
    "          rec_img_vec += self.gray_mean_vec\n",
    "        elif colour == 'red':\n",
    "          rec_img_vec = np.dot(X[:,:n_eigenfaces], self.red_eigenfaces.T[:n_eigenfaces,:])\n",
    "          rec_img_vec += self.red_mean_vec\n",
    "        elif colour == 'green':\n",
    "          rec_img_vec = np.dot(X[:,:n_eigenfaces], self.green_eigenfaces.T[:n_eigenfaces,:])\n",
    "          rec_img_vec += self.green_mean_vec\n",
    "        elif colour == 'blue':\n",
    "          rec_img_vec = np.dot(X[:,:n_eigenfaces], self.blue_eigenfaces.T[:n_eigenfaces,:])\n",
    "          rec_img_vec += self.blue_mean_vec\n",
    "\n",
    "        reconstructed = rec_img_vec.reshape(X.shape[0], self.size[0], self.size[1])\n",
    "        return reconstructed\n",
    "\n",
    "\n",
    "    def plot_eigenfaces(self, img_per_row=10):\n",
    "      if self.colour == False:\n",
    "        self.plot_eigenfaces_gray(img_per_row)\n",
    "      elif self.colour == True:\n",
    "        self.plot_eigenfaces_colour(img_per_row)\n",
    "\n",
    "    def plot_eigenfaces_gray(self,img_per_row = 10):\n",
    "      n_rows = 1+ int(self.gray_eigenfaces.shape[1]/(img_per_row))\n",
    "      n_cols = min(img_per_row, self.gray_eigenfaces.shape[1])\n",
    "\n",
    "      f, ax = plt.subplots(n_rows,n_cols, figsize=(10*n_cols,10*n_rows))\n",
    "\n",
    "      for i ,ax in enumerate(ax.flat):\n",
    "        if i < self.gray_eigenfaces.shape[1]:\n",
    "          ax.imshow(self.gray_eigenfaces[:,i].reshape(self.size[0], self.size[1]))\n",
    "        \n",
    "    def plot_eigenvalues(self):\n",
    "      if self.colour == False:\n",
    "        plt.plot(self.gray_eigenvalues)\n",
    "      else:\n",
    "        f, ax = plt.subplots(1,3, figsize=(10*1,10*3))\n",
    "        ax[0].plot(self.blue_eigenvalues)\n",
    "        ax[1].plot(self.green_eigenvalues)\n",
    "        ax[2].plot(self.red_eigenvalues)\n",
    "\n",
    "    def plot_eigenfaces_colour(self, img_per_row=10):\n",
    "      n_cols = img_per_row\n",
    "      n_rowsR = 1+ int(self.red_eigenfaces.shape[1]/(img_per_row))\n",
    "      n_rowsG = 1+ int(self.green_eigenfaces.shape[1]/(img_per_row))\n",
    "      n_rowsB = 1+ int(self.blue_eigenfaces.shape[1]/(img_per_row))\n",
    "\n",
    "      fig1, ax1 = plt.subplots(n_rowsR,n_cols, figsize=(10*n_cols,10*n_rowsR))\n",
    "      for i, ax1 in enumerate(ax1.flat):\n",
    "        if i < self.red_eigenfaces.shape[1]:\n",
    "          ax1.imshow(self.red_eigenfaces[:, i].reshape(self.size[0], self.size[1]))\n",
    "\n",
    "      fig2, ax2 = plt.subplots(n_rowsG,n_cols, figsize=(10*n_cols,10*n_rowsG))\n",
    "      for i, ax2 in enumerate(ax2.flat):\n",
    "        if i < self.green_eigenfaces.shape[1]:\n",
    "          ax2.imshow(self.green_eigenfaces[:, i].reshape(self.size[0], self.size[1]))\n",
    "\n",
    "      fig3, ax3 = plt.subplots(n_rowsB,n_cols, figsize=(10*n_cols,10*n_rowsB))\n",
    "      for i, ax3 in enumerate(ax3.flat):\n",
    "        if i < self.blue_eigenfaces.shape[1]:\n",
    "          ax3.imshow(self.blue_eigenfaces[:, i].reshape(self.size[0], self.size[1]))\n",
    "\n",
    "    def plot_feature_space_gray(self):\n",
    "      plt.scatter(self.gray_features[np.where(self.labels == 0)[0], 0], self.gray_features[np.where(self.labels==0)[0], 1], c=\"orange\", label=\"MichaelCera\")\n",
    "      plt.scatter(self.gray_features[np.where(self.labels == 1)[0], 0], self.gray_features[np.where(self.labels==1)[0], 1], c=\"red\", label=\"JesseEisenberg\")\n",
    "      plt.scatter(self.gray_features[np.where(self.labels == 2)[0], 0], self.gray_features[np.where(self.labels==2)[0], 1], c=\"blue\", label=\"MilaKunis\")\n",
    "      plt.title(\"Plotted PCA of Grey Images\")\n",
    "      plt.legend()\n",
    "      plt.xlabel('PC1')\n",
    "      plt.ylabel('PC2')\n",
    "      plt.legend()\n",
    "      plt.show()\n",
    "\n",
    "    def plot_feature_space_colour(self):\n",
    "      fig, ax = plt.subplots(1,3)\n",
    "\n",
    "      ax[0].scatter(self.blue_features[np.where(self.labels == 0)[0], 0], self.blue_features[np.where(self.labels == 0)[0], 1], c=\"orange\", label=\"MichaelCera\")\n",
    "      ax[0].scatter(self.blue_features[np.where(self.labels == 1)[0], 0], self.blue_features[np.where(self.labels == 1)[0], 1], c=\"red\", label=\"JesseEisenberg\")\n",
    "      ax[0].scatter(self.blue_features[np.where(self.labels == 2)[0], 0], self.blue_features[np.where(self.labels == 2)[0], 1], c=\"blue\", label=\"MilaKunis\")\n",
    "      ax[0].set_title(\"B Channel\")\n",
    "      ax[0].set_xlabel(\"PC1\")\n",
    "      ax[0].set_ylabel(\"PC2\")\n",
    "\n",
    "      ax[1].scatter(self.red_features[np.where(self.labels == 0)[0], 0], self.red_features[np.where(self.labels == 0)[0], 1], c=\"orange\", label=\"MichaelCera\")\n",
    "      ax[1].scatter(self.red_features[np.where(self.labels == 1)[0], 0], self.red_features[np.where(self.labels == 1)[0], 1], c=\"red\", label=\"JesseEisenberg\")\n",
    "      ax[1].scatter(self.red_features[np.where(self.labels == 2)[0], 0], self.red_features[np.where(self.labels == 2)[0], 1], c=\"blue\", label=\"MilaKunis\")\n",
    "      ax[1].set_title(\"R Channel\")\n",
    "      ax[1].set_xlabel(\"PC1\")\n",
    "      ax[1].set_ylabel(\"PC2\")\n",
    "\n",
    "      ax[2].scatter(self.green_features[np.where(self.labels == 0)[0], 0], self.green_features[np.where(self.labels == 0)[0], 1], c=\"orange\", label=\"MichaelCera\")\n",
    "      ax[2].scatter(self.green_features[np.where(self.labels == 1)[0], 0], self.green_features[np.where(self.labels == 1)[0], 1], c=\"red\", label=\"JesseEisenberg\")\n",
    "      ax[2].scatter(self.green_features[np.where(self.labels == 2)[0], 0], self.green_features[np.where(self.labels == 2)[0], 1], c=\"blue\", label=\"MilaKunis\")\n",
    "      ax[2].set_title(\"G Channel\")\n",
    "      ax[2].set_xlabel(\"PC1\")\n",
    "      ax[2].set_ylabel(\"PC2\")\n",
    "\n",
    "      # The following legend applies to all three subplots.\n",
    "      ax[2].legend(bbox_to_anchor=(1.01, 1), loc='upper left')\n",
    "      plt.tight_layout()\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3dec56",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 1.2.1. Eigenface Plots\n",
    "Let us run PCA analyses to examine the eigenfaces of the images, both on colourscale and on greyscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5590dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T21:36:22.833625Z",
     "iopub.status.busy": "2024-04-14T21:36:22.833119Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's first run colourscale PCAs and plot the eigenfaces.\n",
    "facesize=(100, 100)\n",
    "pca_colour = PCA_AllColourscales_FeatureExtractor(train_X, train_y, 95, facesize, True)\n",
    "features_red, features_green, features_blue = pca_colour.get_features()\n",
    "eigenfaces_red, eigenfaces_blue, eigenfaces_green = pca_colour.get_eigenfaces()\n",
    "pca_colour.plot_eigenfaces(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee3840d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T21:31:58.948581Z",
     "iopub.status.busy": "2024-04-14T21:31:58.947171Z",
     "iopub.status.idle": "2024-04-14T21:31:59.701505Z",
     "shell.execute_reply": "2024-04-14T21:31:59.700041Z",
     "shell.execute_reply.started": "2024-04-14T21:31:58.948530Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now taking a look at the eigenvalues of the model to examine variance in the components\n",
    "pca_colour.plot_eigenvalues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc861e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T21:32:20.130978Z",
     "iopub.status.busy": "2024-04-14T21:32:20.130490Z",
     "iopub.status.idle": "2024-04-14T21:32:35.579525Z",
     "shell.execute_reply": "2024-04-14T21:32:35.577953Z",
     "shell.execute_reply.started": "2024-04-14T21:32:20.130939Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's look at the greyscale version now\n",
    "pca_grey = PCA_AllColourscales_FeatureExtractor(train_X, train_y, 95, facesize, False)\n",
    "features_grey = pca_grey.get_features()\n",
    "eigenfaces_grey = pca_grey.get_eigenfaces()\n",
    "print(eigenfaces_grey.shape)\n",
    "pca_grey.plot_eigenfaces(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de30f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T21:32:41.383492Z",
     "iopub.status.busy": "2024-04-14T21:32:41.383065Z",
     "iopub.status.idle": "2024-04-14T21:32:41.640575Z",
     "shell.execute_reply": "2024-04-14T21:32:41.639473Z",
     "shell.execute_reply.started": "2024-04-14T21:32:41.383464Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Same as before, but greyscale\n",
    "pca_grey.plot_eigenvalues()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a556583",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 1.2.2. Feature Space Plots\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675dae2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T21:32:45.146438Z",
     "iopub.status.busy": "2024-04-14T21:32:45.145971Z",
     "iopub.status.idle": "2024-04-14T21:32:45.975218Z",
     "shell.execute_reply": "2024-04-14T21:32:45.974027Z",
     "shell.execute_reply.started": "2024-04-14T21:32:45.146405Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, let's plot the feature space of the RGB images\n",
    "pca_colour.plot_feature_space_colour()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef1b0fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T21:32:48.970166Z",
     "iopub.status.busy": "2024-04-14T21:32:48.969699Z",
     "iopub.status.idle": "2024-04-14T21:32:49.318107Z",
     "shell.execute_reply": "2024-04-14T21:32:49.316759Z",
     "shell.execute_reply.started": "2024-04-14T21:32:48.970132Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Same thing, but for greyscale\n",
    "pca_grey.plot_feature_space_gray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b80482",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T21:33:03.355453Z",
     "iopub.status.busy": "2024-04-14T21:33:03.354380Z",
     "iopub.status.idle": "2024-04-14T21:33:17.013999Z",
     "shell.execute_reply": "2024-04-14T21:33:17.012521Z",
     "shell.execute_reply.started": "2024-04-14T21:33:03.355404Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What if we try to rebuild the images?\n",
    "pca_colour = PCA_AllColourscales_FeatureExtractor(train_X, train_y, 95, facesize, True)\n",
    "features_red, features_green, features_blue = pca_colour.get_features()\n",
    "n_features = features_red.shape[1]\n",
    "img_per_row = 10\n",
    "n_rows = 1+ int(n_features/(img_per_row))\n",
    "n_cols = min(img_per_row, n_features)\n",
    "f, ax = plt.subplots(n_rows,n_cols, figsize=(10*n_cols,10*n_rows))\n",
    "for i in range(n_features+1):\n",
    "    rec_img = pca_colour.inverse_transform([features_red, features_green, features_blue], [i, i, i])\n",
    "    ax[int(i/img_per_row),int(i%img_per_row)].imshow(np.squeeze(rec_img[0]), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d624a841",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T21:33:25.778977Z",
     "iopub.status.busy": "2024-04-14T21:33:25.778392Z",
     "iopub.status.idle": "2024-04-14T21:33:39.628963Z",
     "shell.execute_reply": "2024-04-14T21:33:39.627164Z",
     "shell.execute_reply.started": "2024-04-14T21:33:25.778936Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Same thing, but now for greyscale\n",
    "pca_grey = PCA_AllColourscales_FeatureExtractor(train_X, train_y, 95, facesize, False)\n",
    "features = pca_grey.get_features()\n",
    "feature_img = features[0,:]\n",
    "n_features = features.shape[1]\n",
    "img_per_row = 10\n",
    "n_rows = 1+ int(features.shape[1]/(img_per_row))\n",
    "n_cols = min(img_per_row, features.shape[1])\n",
    "f, ax = plt.subplots(n_rows,n_cols, figsize=(10*n_cols,10*n_rows))\n",
    "for i in range(n_features+1):\n",
    "    rec_img = pca_grey.inverse_transform(feature_img[np.newaxis,:],i)\n",
    "    ax[int(i/img_per_row),int(i%img_per_row)].imshow(np.squeeze(rec_img), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a238534",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 1.2.3. Discussion\n",
    "In this section, the principal components for the training images (both in colourscale and in greyscale) were found by converting the images to 2D arrays and performing PCA.\n",
    "\n",
    "The original images were 3D as they contained stacked colour channel images. Thus, to obtain greyscale images, the opencv package was used to scale the pixel intensities so that the pixel values only ranged between white and black, which can be contained in a single value. When performing PCA on coloured images, the principal components were actually found on the three separate colour channels. The separated colour channels were obtained, again, through using opencv, this time to split the images into BGR channels. In this manner, 2D matrices representing the original images were obtained for PCA.\n",
    "\n",
    "In preparing the image data for PCA, all the images underwent mean subtraction as a form of data normalization. This normalization step is important in ensuring that features with the maximum variance are created. If the data is not normalized, then the mathematical process may favor features that appear to maximize variance but, in relation to the mean, do not provide as informative components.\n",
    "\n",
    "The primary components capturing the most variation in the data are termed eigenfaces. These eigenfaces arise from determining the eigenvectors of the covariance matrix derived from the 2D matrix. However, in the case of an image size of 100x100, the resulting covariance matrix is massive, at 10000x10000, yet due to the constraints of training data, only 79 eigenvectors possess non-zero eigenvalues. Fortunately, we can exploit the structure of the data matrix to reduce computational complexity and isolate the non-zero eigenvectors. This involves first computing the covariance matrix of the transposed data matrix, now reduced to 80x80. Subsequently, we compute the eigenvectors and eigenvalues of this covariance matrix. Multiplying these eigenvectors with the original 2D matrix yields the eigenfaces of the training data. The eigenvalues provide insights into which eigenfaces capture the most variance within our training dataset.\n",
    "\n",
    "The optimal value of the number of principal components must retain a percentage of the information that is representative and at the same time significantly reduces the dimensionality of the problem. Therefore, a trade-off between information and dimensionality has to be found.\n",
    "\n",
    "Examining the eigenvalues derived from PCA exposes the variance encapsulated by individual components. Notably, in our dataset, the initial 25 eigenfaces account for 90% of the total variance, indicating their significant information content. Subsequent eigenfaces contribute progressively less to the overall retained information. This trend becomes evident when observing the reconstruction of the initial image, where the resemblance to the original remains pronounced.\n",
    "Already with the first 10 eigenfaces (that capture 77% of the total variance) you can start to recognize Mila Kunis in the image.\n",
    "\n",
    "Though the classes appear to occupy slightly different areas in the plotted PCA feature maps, there is quite a bit of overlap. This indicates that the classes may not be easily separable and that using principal components from these PCAs as input may not result in the most robust models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2687c827",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94b815dc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "1. # 2. Evaluation Metrics\n",
    "## 2.0. Example: Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6383bba1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dd7f44",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 3. Classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2294507",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3.1 KNN Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d42b83",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KNNCosineClassification():\n",
    "\n",
    "  def __init__(self, knn=3):\n",
    "    self.classifier = KNeighborsClassifier(n_neighbors=knn,metric=\"cosine\")\n",
    "\n",
    "  def fit(self, train_features, y_train):\n",
    "    self.classifier.fit(train_features,y_train)\n",
    "\n",
    "  def predict(self, x_val):\n",
    "    return self.classifier.predict(x_val)\n",
    "\n",
    "  def __call__(self,x_val):\n",
    "    return self.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181813c6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KNNClassification():\n",
    "\n",
    "  def __init__(self, knn=10):\n",
    "    self.classifier = KNeighborsClassifier(n_neighbors=knn)\n",
    "\n",
    "  def fit(self, train_features, y_train):\n",
    "    self.classifier.fit(train_features,y_train)\n",
    "\n",
    "  def predict(self, x_val):\n",
    "    return self.classifier.predict(x_val)\n",
    "\n",
    "  def __call__(self,x_val):\n",
    "    return self.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e596bef",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3.2 Decision tree classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d313e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "\n",
    "    def __init__(self, random_state):\n",
    "        self.classifier = DecisionTreeClassifier(random_state=random_state)\n",
    "\n",
    "    def fit(self, train_features, y_train):\n",
    "        self.classifier.fit(train_features, y_train)\n",
    "\n",
    "    def predict(self, x_val):\n",
    "        return self.classifier.predict(x_val)\n",
    "\n",
    "    def __call__(self,x_val):\n",
    "        return self.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27ac66e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3.3 SVM classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4050f035",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SVM():\n",
    "    def __init__(self):\n",
    "        self.classifier = LinearSVC()\n",
    "\n",
    "    def fit(self, train_features, y_train):\n",
    "        self.classifier.fit(train_features, y_train)\n",
    "\n",
    "    def predict(self, x_val):\n",
    "        return self.classifier.predict(x_val)\n",
    "\n",
    "    def __call__(self,x_val):\n",
    "        return self.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb5a17",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3.4. Baseline: VGG16 transfer learn model on original images\n",
    "We first took the head of a VGG16 model and experimented with the number of hidden layers and number of neurons in the network. We did this to have a baseline of what a model could do without hand crafted features. The baseline achieved was 81.1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c77ae",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, losses, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f5b898",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OriginalImageClassificationModel:\n",
    "    def __init__(self):\n",
    "        base_model = tf.keras.applications.VGG16(weights = 'imagenet', include_top = False, input_shape = (100,100,3)) # have you considered using VGG19? https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG19\n",
    "        for layer in base_model.layers:\n",
    "          layer.trainable = False\n",
    "        x = layers.Flatten()(base_model.output)\n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        predictions = layers.Dense(3, activation = 'softmax')(x)\n",
    "        self.model = Model(inputs = base_model.input, outputs = predictions)\n",
    "        self.model.compile(optimizer='adam', loss=losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "    def fit(self, x_train, y_train, x_val, y_val, batch_size=64, epochs=40):\n",
    "        history = self.model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val, y_val))\n",
    "        return history\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1dd52",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3.5 HOG-image based\n",
    "In the literature we found that models using a HOG-image as input were able to perform well, so therefore we tested some models that could work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86d5daa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.5.1 Using a VGG16 pretrained model\n",
    "The first model we tried was a VGG16 model as it was easy to implement. The problem with this is that VGG16 model requires RGB input so you need to stack 3 HOG-images before feeding it into the model.\n",
    "\n",
    "We tested many different things such as changing the amount of linear layers, changing the number of neurons, batch size, and learning rate. In the end it got close to the baseline but could not surpass it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a4e992",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, losses, Model, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4edd19f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HOGClassificationModel:\n",
    "    def __init__(self):\n",
    "        base_model = tf.keras.applications.VGG16(weights = 'imagenet', include_top = False, input_shape = (100,100,3))\n",
    "        for layer in base_model.layers:\n",
    "          layer.trainable = False\n",
    "        x = layers.Flatten()(base_model.output)\n",
    "        x = layers.Dense(4096, activation='relu')(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        x = layers.Dense(4096, activation='relu')(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        predictions = layers.Dense(3, activation = 'softmax')(x)\n",
    "        self.model = Model(inputs = base_model.input, outputs = predictions)\n",
    "        self.model.compile(optimizer='adam', loss=losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "    def fit(self, x_train, y_train, x_val, y_val, batch_size=12, epochs=40):\n",
    "        history = self.model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val, y_val))\n",
    "        return history\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca579a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.4.2 Custom model\n",
    "In a paper we found a model that performed really well on benchmark datasets. There for we copied the model and did some tests with it. We unfortunately were not able to replicate the good results on our dataset. In the paper they used 48x48 images so we tested it on both 48x48 and 100x100 and didn't notice a difference.\n",
    "\n",
    "The problem with this model is that it was always able do well on the training set but it didn't transfer to the validation set. We suspect it was overfitting so we tried many different things to fix it, such as testing many scaled down models, augmenting the training data, regenerating the augmlented data every couple of epochs, and adding dropout layers (also between the conv2s layers).\n",
    "\n",
    "After testing all these things we decided to give up on only HOG-image based deep learning classifiers\n",
    "\n",
    "Source: Benradi, Hicham, Ahmed Chater, and Abdelali Lasfar. \"A hybrid approach for face recognition using a convolutional neural network combined with feature extraction techniques.\" IAES Int J Artif Intell 12.2 (2023): 627."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b7750c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HOGClassificationModel2:\n",
    "    def __init__(self):\n",
    "        self.model = models.Sequential()\n",
    "        self.model.add(layers.Conv2D(6, 5, activation='relu', input_shape = (48,48,1)))\n",
    "        self.model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(layers.Conv2D(16, 5, activation='relu'))\n",
    "        self.model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(layers.Conv2D(64, 3, activation='relu'))\n",
    "        self.model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(layers.Flatten())\n",
    "        self.model.add(layers.Dense(256, activation='relu'))\n",
    "        self.model.add(layers.Dropout(0.5))\n",
    "        self.model.add(layers.Dense(3, activation = 'sigmoid'))\n",
    "        self.model.compile(optimizer=optimizers.RMSprop(), loss=losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    def fit(self, x_train, y_train, x_val, y_val, batch_size=32, epochs=40):\n",
    "        history = self.model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val, y_val))\n",
    "        return history\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dc7983",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3.5 Combined original images and HOG-image approach\n",
    "In the end we decided to combine the previous models into one. So a VGG16 model that extracts features from the original image and the custom HOG-image model that extracts features from the HOG-image. These features are then combined into and given as input to 2 linear layers. After tweeking the network, we were finally able to beat the baseline model. Adding a HOG-image as input imrpoves the standard VGG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cd9282",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, losses, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebebc02e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "!!! Disclaimer you have to parse a dict with the name of the input layer in the fit and predict function and depending on the amount of models that have been created these values change. When it is ran for the first time it are these values other wise they will be different.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4288aa96",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageAndHogClassificationModel:\n",
    "    def __init__(self):\n",
    "        base_model = tf.keras.applications.VGG16(weights = 'imagenet', include_top = False, input_shape = (100,100,3)) # have you considered using VGG19? https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG19\n",
    "        for layer in base_model.layers:\n",
    "          layer.trainable = False\n",
    "\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(6, 5, activation='relu', input_shape = (100,100,1)))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Conv2D(16, 5, activation='relu'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Conv2D(64, 3, activation='relu'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Flatten())\n",
    "\n",
    "        y = layers.Flatten()(base_model.output)\n",
    "        z = layers.concatenate([y, model.output])\n",
    "\n",
    "        x = layers.Dense(1024, activation='relu')(y)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        x = layers.Dense(1024, activation='relu')(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        predictions = layers.Dense(3, activation = 'softmax')(x)\n",
    "        self.model = Model(inputs = [base_model.input, model.input] , outputs = predictions)\n",
    "        self.model.compile(optimizer='adam', loss=losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "    def fit(self, x_train1, x_train2, y_train, x_val1, x_val2, y_val, batch_size=64, epochs=40):\n",
    "        history = self.model.fit({'input_1': x_train1, \"conv2d_input\": x_train2}, y_train, batch_size=batch_size, epochs=epochs, validation_data=({'input_1': x_val1, \"conv2d_input\": x_val2}, y_val))\n",
    "        return history\n",
    "\n",
    "    def predict(self, X1, X2):\n",
    "        return self.model.predict({'input_1': X1, \"conv2d_input\": X2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73a92b8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 4. Experiments\n",
    "\n",
    "## 4.0. SIFT pipeline\n",
    "\n",
    "First some experiments using SVM and KNN on the SIFT feature space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d649efe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"Octaves\":4,\n",
    "    \"NumberOfFeatures\": 100,\n",
    "    \"ConstrastThreshold\": 0.01,\n",
    "    \"EdgeThreshold\": 30,\n",
    "    \"Smoothing\": 2.0,\n",
    "}\n",
    "\n",
    "classifier = SVM()\n",
    "skf = StratifiedKFold(n_splits = 10)\n",
    "mean_acc = []\n",
    "\n",
    "for train_idx, test_idx in skf.split(new_train_X, new_train_y):\n",
    "    x_train, y_train = new_train_X[train_idx,:,:,:], new_train_y[train_idx]\n",
    "    x_val, y_val = new_train_X[test_idx,:,:,:], new_train_y[test_idx]\n",
    "    trans = SIFT_FeatureExtractor(**PARAMS)\n",
    "    all_keypoints, all_features = trans(x_train) #Note that these are LISTS with a length equal to the training set size\n",
    "    trans.createKWords(all_features,numClusters)\n",
    "    all_words = trans.getWords(all_features)\n",
    "    classifier.fit(all_words, y_train)\n",
    "\n",
    "    test_keypoints, test_features = trans(x_val)\n",
    "    test_words = trans.getWords(test_features)\n",
    "\n",
    "    y_pred = classifier(test_words)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    mean_acc.append(acc)\n",
    "    print(acc)\n",
    "print(\"Accuracy from 10-fold cross validation on SIFT features in SVM classifier: \",np.mean(mean_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ad31d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"Octaves\":4,\n",
    "    \"NumberOfFeatures\": 100,\n",
    "    \"ConstrastThreshold\": 0.01,\n",
    "    \"EdgeThreshold\": 30,\n",
    "    \"Smoothing\": 2.0,\n",
    "}\n",
    "\n",
    "classifier = KNNCosineClassification()\n",
    "skf = StratifiedKFold(n_splits = 10)\n",
    "mean_acc = []\n",
    "\n",
    "for train_idx, test_idx in skf.split(new_train_X, new_train_y):\n",
    "    x_train, y_train = new_train_X[train_idx,:,:,:], new_train_y[train_idx]\n",
    "    x_val, y_val = new_train_X[test_idx,:,:,:], new_train_y[test_idx]\n",
    "    trans = SIFT_FeatureExtractor(**PARAMS)\n",
    "    all_keypoints, all_features = trans(x_train) #Note that these are LISTS with a length equal to the training set size\n",
    "    trans.createKWords(all_features,numClusters)\n",
    "    all_words = trans.getWords(all_features)\n",
    "    classifier.fit(all_words, y_train)\n",
    "\n",
    "    test_keypoints, test_features = trans(x_val)\n",
    "    test_words = trans.getWords(test_features)\n",
    "\n",
    "    y_pred = classifier(test_words)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    mean_acc.append(acc)\n",
    "    print(acc)\n",
    "print(\"Accuracy from 10-fold cross validation on SIFT features in KNN classifier: \",np.mean(mean_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ce7c7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_extractor = IdentityFeatureExtractor() \n",
    "classifier = RandomClassificationModel()\n",
    "\n",
    "# train the model on the features\n",
    "classifier.fit(feature_extractor(train_X), train_y)\n",
    "\n",
    "# model/final pipeline\n",
    "model = lambda X: classifier(feature_extractor(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f6e37",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate performance of the model on the training set\n",
    "train_y_star = model(train_X)\n",
    "\n",
    "\"The performance on the training set is {:.2f}. This however, does not tell us much about the actual performance (generalisability).\".format(\n",
    "    accuracy_score(train_y, train_y_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d2d76",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict the labels for the test set \n",
    "test_y_star = model(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff96d49",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 4.1 HOG-image pipeline\n",
    "With changing the augmented data every couple of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde9f2f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Extract the HOG-images without stacking them\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182e635c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hogfe = HOGFeatureExtractor()\n",
    "\n",
    "hog_train_X = []\n",
    "\n",
    "for i in range(len(train_X)):\n",
    "  _, hog_image = hogfe.transform(train_X[i])\n",
    "  hog_train_X.append([hog_image])\n",
    "hog_train_X = np.array(hog_train_X).transpose(0,2,3,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6401d7a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "A funciton to convert a whole list of images to HOG-images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2541620",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hog_trans_list(train_list):\n",
    "  hogfe = HOGFeatureExtractor()\n",
    "  temp = []\n",
    "  for i in range(len(train_list)):\n",
    "    _, hog_image = hogfe.transform(train_list[i])\n",
    "    temp.append([hog_image])\n",
    "  return np.array(temp).transpose(0,2,3,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db972df",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Extract the HOG-images without stacking them\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89959276",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hogfe = HOGFeatureExtractor()\n",
    "\n",
    "hog_train_X = []\n",
    "\n",
    "for i in range(len(new_train_X)):\n",
    "  _, hog_image = hogfe.transform(new_train_X[i])\n",
    "  hog_train_X.append(np.repeat(hog_image[..., np.newaxis], 3, -1))\n",
    "hog_train_X = np.array(hog_train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b8d40",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "A funciton to convert a whole list of images to HOG-images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b876c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hog_trans_list(train_list):\n",
    "  hogfe = HOGFeatureExtractor()\n",
    "  temp = []\n",
    "  for i in range(len(train_list)):\n",
    "    _, hog_image = hogfe.transform(train_list[i])\n",
    "  hog_train_X.append(np.repeat(hog_image[..., np.newaxis], 3, -1))\n",
    "  return np.array(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccdd0d0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Choose the model you want to\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7903274",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = HOGClassificationModel2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6830a334",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Train the model with K-fold validation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed54283",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca46d26",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = 4)\n",
    "for train_idx, test_idx in skf.split(train_X, train_y):\n",
    "    x_val, y_val = train_X[test_idx,:,:,:], train_y[test_idx]\n",
    "    x_val, y_val = apply_augmentations(x_val, y_val)\n",
    "    x_val = hog_trans_list(x_val)\n",
    "    for _ in range(3):\n",
    "      x_train, y_train = train_X[train_idx,:,:,:], train_y[train_idx]\n",
    "      x_train, y_train = apply_augmentations(x_train, y_train)\n",
    "      x_train = hog_trans_list(x_train)\n",
    "      history = model.fit(x_train, y_train, x_val, y_val, batch_size=64, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0ca2d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Extract the HOG-images from the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e53134f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hogfe = HOGFeatureExtractor()\n",
    "\n",
    "hog_test_X = []\n",
    "\n",
    "for i in range(len(test_X)):\n",
    "  _, hog_image = hogfe.transform(test_X[i])\n",
    "  hog_test_X.append([hog_image])\n",
    "hog_test_X = np.array(hog_test_X).transpose(0,2,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a725e4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hogfe = HOGFeatureExtractor()\n",
    "\n",
    "hog_test_X = []\n",
    "\n",
    "for i in range(len(test_X)):\n",
    "  _, hog_image = hogfe.transform(test_X[i])\n",
    "  hog_test_X.append(np.repeat(hog_image[..., np.newaxis], 3, -1))\n",
    "hog_test_X = np.array(hog_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d09ba",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_y_star = model.predict(hog_test_X)\n",
    "test_y_star = np.argmax(test_y_star, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae40266",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 4.2 HOG-image and normal images pipeline\n",
    "With changing the augmented data every couple of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c98bae",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Use the HOG extraction functions from the previous pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d15ca0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Choose your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37bdb1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ImageAndHogClassificationModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7afcf95",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b1a41",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c398c6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = 4)\n",
    "for train_idx, test_idx in skf.split(train_X, train_y):\n",
    "    x_val1, y_val = train_X[test_idx,:,:,:], train_y[test_idx]\n",
    "    x_val1, y_val = apply_augmentations(x_val1, y_val)\n",
    "    x_val2 = hog_trans_list(x_val1)\n",
    "    for _ in range(1):\n",
    "      x_train1, y_train = train_X[train_idx,:,:,:], train_y[train_idx]\n",
    "      x_train1, y_train = apply_augmentations(x_train1, y_train)\n",
    "      x_train2 = hog_trans_list(x_train1)\n",
    "      history = model.fit(x_train1, x_train2, y_train, x_val1, x_val2, y_val, batch_size=128, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665b1aa2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Extract the HOG-images from the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa73fd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hogfe = HOGFeatureExtractor()\n",
    "\n",
    "hog_test_X = []\n",
    "\n",
    "for i in range(len(test_X)):\n",
    "  _, hog_image = hogfe.transform(test_X[i])\n",
    "  hog_test_X.append([hog_image])\n",
    "hog_test_X = np.array(hog_test_X).transpose(0,2,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f03e641",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hogfe = HOGFeatureExtractor()\n",
    "\n",
    "hog_test_X = []\n",
    "\n",
    "for i in range(len(test_X)):\n",
    "  _, hog_image = hogfe.transform(test_X[i])\n",
    "  hog_test_X.append(np.repeat(hog_image[..., np.newaxis], 3, -1))\n",
    "hog_test_X = np.array(hog_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d13118",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_y_star = model.predict(test_X, hog_test_X)\n",
    "test_y_star = np.argmax(test_y_star, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6016d2c1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 5. Publishing best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2ef02c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = test.copy().drop('img', axis = 1)\n",
    "submission['class'] = test_y_star\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050b6648",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799cc16b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 6. Discussion\n",
    "\n",
    "\n",
    "The main lessons we have learned during this project are: data cleaning is a very important step and should be done at the beginning. \n",
    "While this was a relatively less arduous task given the size of the dataset, it adds layers of complications to the data loading mechanism.\n",
    "Verifying and re-verifying that the pre-processors work as intended is key. \n",
    "Here we have only slightly tweaked the pre-processor, but that is not a robust solution since there is some degree of uncertainty linked to the order in which the faces are detected by the pre-processor.\n",
    "Handcrafted features can be used to improve upon the standard CNN-based models.\n",
    "However, handcrafted features require careful finetuning and consideration for what image aspects are considered discriminative for the task at hand and will generalise to unseen data.\n",
    "There is a big difference between model performance on the test set versus using cross-validation on the training set\n",
    "\n",
    "We believe the Nearest Neighbour model struggled with handcrafted features due to the limited ability to extrapolate to unseen data. While SVM worked better, it tended to overfit to the noise and the pictures of Sarah Nyland and Michael Cera, which was made clear by the poor performance on the validation and test sets. PCA may be more appropriate for data compression, tasks where interpretability is important, and situations with limited computational resources. SIFT provides an excellent framework for larger object detection but struggles with identifying finer details for a discriminative classifier. It is very good at separating Jesse from Mila, but not good at identifying the lookalikes.\n",
    "\n",
    "\n",
    "If we had more time, we would implement pre-processing pipelines that have shown better results in similar cases. We also didnt have the time to study the effect of each augmentation on the overall accuracy of models and we might have missed some useful augmentations. For our future classifiers, we would Investigate ensemble methods combining learned features with handcrafted features, as HOG and normal images together give better results. For Nearest Neighbour, not only looking at the closest labels but also looking at the total distance to the nearest Neighbour might improve the classifier. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7751321,
     "sourceId": 70926,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-14T21:57:45.901346",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}